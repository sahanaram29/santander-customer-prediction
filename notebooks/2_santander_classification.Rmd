---
title: "Santander_classification"
author: "Sahana Ramakrishnan"
date: "2024-11-29"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(randomForest)
library(xgboost)
library(rpart)
library(pROC)
```

## Classification Task on binary response variable from Santander data set

### Data Loading
```{r}
# training and test datasets loading
train <- read_csv("D:/NewEng/Fall - Winter 2024/Santander case study/data set/train.csv")
test <- read_csv("D:/NewEng/Fall - Winter 2024/Santander case study/data set/test.csv")

#Structure of train set
#summary(train)
```
- Checked loading the training data set

### Dimensionality reduction using PCA
```{r}
# Split predictors and target
train_features <- train %>% select(-ID_code, -target)
train_target <- train$target
test_features <- test %>% select(-ID_code)

# Standardize predictors
train_features_scaled <- scale(train_features)
test_features_scaled <- scale(test_features, center = attr(train_features_scaled, "scaled:center"), 
                               scale = attr(train_features_scaled, "scaled:scale"))

# PCA for dimensionality reduction
pca <- prcomp(train_features_scaled, center = TRUE, scale. = TRUE)
# retaining top 50 components
train_pca <- as.data.frame(pca$x[, 1:50]) 
train_pca$target <- train_target
test_pca <- as.data.frame(predict(pca, test_features_scaled)[, 1:50])
```

- The ID_code and target columns are separated to isolate predictors (train_features) and the binary response variable (train_target)
- The predictors are standardized to ensure all features contribute equally to PCA, preventing bias from features with larger ranges
- PCA is applied to the scaled predictors and the top 50 components which explain most of the variance in the data are retained for both the training and test datasets. 
- This significantly reduces the dimensionality while preserving the majority of the dataset's information. 
The binary target variable is then reattached to the transformed train_pca

### permutation 1: Baseline Logistic Regression
```{r}
log_model1 <- glm(target ~ ., data = train_pca, family = binomial)
summary(log_model1)

# Predictions
log_pred1 <- predict(log_model1, newdata = train_pca, type = "response")
log_pred_class1 <- ifelse(log_pred1 > 0.5, 1, 0)

# Metrics
conf_matrix1 <- table(Predicted = log_pred_class1, Actual = train_pca$target)
cat("Accuracy:", sum(diag(conf_matrix1)) / sum(conf_matrix1), "\n")
```
- The logistic regression model with PCA-transformed features achieved an accuracy of 91.4% with several principal components like PC1, PC8, PC41 showing significant contributions to predicting the target variable. 
- It shows strong performance but further tuning or evaluation might refine results.

### logistic - permutation 2: adjusting threshold
```{r}
# Change threshold to 0.3
log_pred_class2 <- ifelse(log_pred1 > 0.3, 1, 0)  
conf_matrix2 <- table(Predicted = log_pred_class2, Actual = train_pca$target)
cat("Accuracy with threshold 0.3:", sum(diag(conf_matrix2)) / sum(conf_matrix2), "\n")

```
- By lowering the decision threshold to 0.3 in logistic regression the model achieved an accuracy of 90.2%. 
- This indicates that the threshold adjustment slightly reduced overall accuracy likely balancing between sensitivity and specificity for better handling of class imbalance.

### XG Boost: Permutation 1

```{r}
dtrain <- xgb.DMatrix(data = as.matrix(train_pca %>% select(-target)), label = as.numeric(as.factor(train_pca$target)) - 1)

xgb_params1 <- list(objective = "binary:logistic", eval_metric = "auc", max_depth = 6, eta = 0.1)
xgb_model1 <- xgb.train(params = xgb_params1, data = dtrain, nrounds = 100)
xgb_model1

```
- The XGBoost model was trained with 50 PCA components as features and 100 iterations, using parameters such as a maximum depth of 6 and a learning rate of 0.1

### Adjusting hyperparameters
```{r}
xgb_params2 <- list(objective = "binary:logistic", eval_metric = "auc", max_depth = 8, eta = 0.05)
xgb_model2 <- xgb.train(params = xgb_params2, data = dtrain, nrounds = 200)
xgb_model2
```
- The tuned XGBoost model was trained with 50 PCA components as features and 200 iterations using a deeper tree with max_depth = 8 and a reduced learning rate of 0.05). 
- These adjustments aim to improve model performance by capturing more complex patterns while ensuring gradual learning to avoid overfitting.

### Model Comparison
```{r}
# collect model performance metrics
performance <- data.frame(
  Model = c("Logistic Regression (Baseline)", "Logistic Regression (Threshold 0.3)", 
            "XGBoost (Baseline)", "XGBoost (Tuned)"),
  Accuracy = c(
    sum(diag(conf_matrix1)) / sum(conf_matrix1),
    sum(diag(conf_matrix2)) / sum(conf_matrix2),
    auc(roc(train_pca$target, predict(xgb_model1, as.matrix(train_pca %>% select(-target))))),
    auc(roc(train_pca$target, predict(xgb_model2, as.matrix(train_pca %>% select(-target)))))
  )
)

performance

```


- The model comparison shows that the tuned XGBoost model achieved the highest accuracy of 93.65%, outperforming both the baseline XGBoost and logistic regression models. 
- While baseline logistic regression performed well with 91.4%, tuning XGBoostâ€™s hyperparameters significantly improved performance making it the best model for this classification task.